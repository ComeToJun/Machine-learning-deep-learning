데이터 분석

1. 수집
크롤링 
설문조사
기존 데이터 베이스

<적재 저장>

2. 전처리
데이터 타입 지정 Dtype
토큰 Token
이상치파악, 결측치 파악

데이터 마트 구축

3. 데이터 분석
Info, summary
비지니스 파악 ( 전문가 의견 ) - 특성 조합
산포도
시퀀스 ( 시간에 따른 데이터의 변화 )
상관관계 파악

4. 데이터 편집, 변환, 파이프라인 구축
Train / Test 나누기 ( 여러 방식으로 가능 )
   8  /   2    정도로 나누는게 보편
  
이상치 처리, 결측치 처리, 스케일링, 원핫인코딩, 특성조합 
- Train set 기준 ( 파이프라인으로 관리)
- 변환된 값을 Test 에 적용

5. 모델 선정 및 훈련 ( 시간이 가장 많이 걸림 )
- 데이터를 준비했으니 컴퓨터에서 맡기는 구간

GridSerarchCV

6. Test

5번에서 나온 Best 결과값을 4번에서 만든 Test 에 적용해서 마지막 결과값을 가져온다.
(Test 는 쓰지 말고, 마지막까지 기다렸다가 적용)




feature 문제, 특성
label y 값

y = f(x)

회귀
연속적인 실수를 예측한다.

훈련된데이터로 검증을 잘하면 과대적합
데이터를 복잡하게 하려고 하면, 과대적합

훈련데이터나 성능데이터가 잘 안나오면 과소적합
극단적으로 하나 혹은 두개의 기준만으로 판단해서 결과를 내려고 하면 과소적합

오차는 생길 수 밖에 없다.

그 사이값을 잘 찾는것을 일반화 라고 한다.

100% 를 목표치로 잡으면 과대적합이 될 수 밖에 없다.




머신러닝은 예측을 통한 데이터 분석의 도구, 학문
목적 중 하나는 데이터 마이닝
사람이 할 일은 머신이 잘 분석할 수 있도록 분류해서 머신에게 넘겨주는 것 말고는 없다.


과대적합은 규칙이 너무 많고 복잡해지기 때문에 민감해져 일반화가 어렵다
과소적합은 규칙이 적용되는 범위가 너무 넓어 모델의 신뢰도가 떨어진다.

데이터의 양이 데이터의 다양성을 키워준다. 데이터가 많을 수록 더 적절하게 복잡하고 신뢰성 있는 모델을 만들 수 있다.



머신러닝은 추세나 패턴을 보는 기준
100% 를 생각하면 안된다.

k-nn 은 이웃의 개수가 많을수록 복잡도가 낮아지고
수가 적을수록 복잡도가 높아진다.



머신러닝에서 가장 중요한건 데이터의 모양
훈련시트 데이터와 테스트 데이터의 형상(shape)은 언제나 일치해야 한다.


merge concat 에서 데이터의 개수가 중요하다.

열의 개수만 맞으면 일단 들어간다.
(행, 열)
강제로 모양을 맞춰서 넣기도 한다.
머신러닝에서는 열의 모양을 맞추는걸 목표로한다.

앞쪽은 데이터 샘플의 개수
뒤쪽은 데이터 특성의 개수
뒤쪽만 맞으면 된다.




회귀
연속적인 실수를 예측한다.

y = target  정답
y^ = predict  예측
y- = Targetmean  y들의 평균



선형회귀의 목적

퍼져있는 점들을 선으로 이었을때 절편과 기울기를 잘 표현할 수 있는 것을 찾는 것.

MSE
오차를 재기 위한 성능지표

y = wx + b
w = 기울기 (x가 변화했을때 y는 얼마나 변했는가)
b = 절편

y 축 오차
x 축 w

w (가중치) 를 조절하면 오차를 조절 할 수 있다.

선형회귀의 목적
w 를 조절해서 오차를 최소화 할 수 있다.
기울기에 따라가 오차가 바뀐다.



미분 : 변화량
미래값 - 현재값 = 변화값 ( 근데 그 변화값이 매우 작다 = 순간적인 변화량을 보겠다 )
* w 기울기와 비슷한 느낌이라는 것을 느낄 수 있다.

미분을 통해서 움직일 방향과 정도를 알 수 있다.

기울기를 찾아가는 방법

MSE 에서 나온 그래프의 기울기를 보고 오차를 줄여가기 위해서 미분이 필요하다
기울이과 절편의 최적값을 찾기 위해서 미분을 사용함


# 선형회귀는 특성이 많으면 많을수록 좋다.
# 가중치 w 가 높을수록 복잡도는 올라간다.


Ridge > Lasso 
MSE       MAE

선형회기에서는 주어진 W 값을 바꿀 수 없는데
패널티를 줄 수 있어서  W 값에 변화를 줄 수 있다.


# 복잡도가 올라가면서 복잡도가 높아진다
# 릿지에서는 패널티를 낮추는게 더 좋다.
# 릿지는 가중치를 0 에 가깝게 만들지만 0 으로 만들진 않는다
# 라쏘는 가중치를 0 으로 만들 수 있다.




1. 일반화 - 과대적합, 과소적합을 어떻게 피할것인가.
나름대로의 일반화를 정의해보자

과대적합 = 가진 정보를 모두 사용해서 너무 복잡한 모델을 만드는 것, training data에만 최적화 된 모델
과소적합 = 너무 간단한 모델이 선택되는 것 


모델을 복잡하게 할 수록 훈련 데이터에 대해서는 더 정확히 예측할 수 있다. 
그러나 너무 복잡해지면 훈련 세트의 각 데이터 포인트에 너무 민감해져 새로운 데이터에 잘 일반화되지 못한다.

우리가 찾으려는 모델은 일반화 성능이 최대가 되는 최적점에 있는 모델


일반화는 테스트 데이터에 대한 높은 성능을 갖추는 것.
즉, 테스트 데이터를 입력했을때, output의 정확도가 높은 것을 의미함



2. 선형회귀의 공식
y = Wx + b
W = [w1, w2, w3]
X = [x1, x2, x3]

3. 미분은 간단히

4. 릿지, 라쏘 -> 가중치 조절

5. MSE 

